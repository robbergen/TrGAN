{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single membership attack on TGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "<ul>\n",
    "  <li>Data pipeline</li>\n",
    "  <li>Adversary network</li>\n",
    "  <li>Loss</li>\n",
    "  <li>Updater</li>\n",
    "  <li>Read in Generator</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "src_path = 'C:/Users/robbe/Downloads/hecktor_train_v3/hecktor-main/src/'\n",
    "sys.path.append(src_path)\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import sys\n",
    "from data import utils\n",
    "from nibabel.testing import data_path\n",
    "import ImageViewer3D as I3D\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import imp\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import chainer\n",
    "from chainer import backend\n",
    "from chainer import backends\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, FunctionNode, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, initializers, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "from chainer.training import extensions\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import cv2 as cv\n",
    "from chainer import Variable\n",
    "from chainer import serializers\n",
    "import cupy\n",
    "\n",
    "src_path = 'C:/Users/robbe/tgan-master/tgan-master/'\n",
    "sys.path.append(src_path)\n",
    "import infer\n",
    "import math\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_imgs = np.load('C:/Users/robbe/tgan-master/tgan-master/cropped_imgs.npy')\n",
    "train = np.expand_dims(cr_imgs[16:-16,1,:,:],axis=1)\n",
    "train = np.transpose(train,[1,0,2,3]) #shape is (n,z,x,y)\n",
    "\n",
    "fsgen_weights = np.load('C:/Users/robbe/tgan-master/tgan-master/results/PrivacyTest-last20witheld-unconditional/gen_iter_28000.npz')\n",
    "vgen_weights = np.load('C:/Users/robbe/tgan-master/tgan-master/results/PrivacyTest-last20witheld-unconditional/vgen_iter_28000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 1\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "#test_iter = iterators.SerialIterator(test, batchsize, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "\n",
    "    def __init__(self,bottom_width = 4, conv_ch = 512):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            \n",
    "            self.l1 = L.Linear(64*64*32,1024,initialW = chainer.initializers.Uniform(0.01))\n",
    "            self.l2 = L.Linear(1024,100,initialW = chainer.initializers.Uniform(0.01))\n",
    "            \n",
    "            self.dc0 = L.DeconvolutionND(1, 100, 1024, 1, 1, 0, initialW=fsgen_weights['dc0/W'],initial_bias=fsgen_weights['dc0/b'])\n",
    "            self.dc1 = L.DeconvolutionND(1, 1024, 512, 4, 2, 1, initialW=fsgen_weights['dc1/W'],initial_bias=fsgen_weights['dc1/b'])\n",
    "            self.dc2 = L.DeconvolutionND(1, 512, 256, 4, 2, 1, initialW=fsgen_weights['dc2/W'],initial_bias=fsgen_weights['dc2/b'])\n",
    "            self.dc3 = L.DeconvolutionND(1, 256, 128, 4, 2, 1, initialW=fsgen_weights['dc3/W'],initial_bias=fsgen_weights['dc3/b'])\n",
    "            self.dc4 = L.DeconvolutionND(1, 128, 128, 4, 2, 1, initialW=fsgen_weights['dc4/W'],initial_bias=fsgen_weights['dc4/b'])\n",
    "            self.dc5 = L.DeconvolutionND(1, 128, 100, 4, 2, 1, initialW=fsgen_weights['dc5/W'],initial_bias=fsgen_weights['dc5/b'])\n",
    "            self.bn0 = L.BatchNormalization(1024, use_beta=False,initial_gamma=fsgen_weights['bn0/gamma'], initial_avg_mean=fsgen_weights['bn0/avg_mean'], initial_avg_var=fsgen_weights['bn0/avg_var'])\n",
    "            self.bn1 = L.BatchNormalization(512, use_beta=False,initial_gamma=fsgen_weights['bn1/gamma'], initial_avg_mean=fsgen_weights['bn1/avg_mean'], initial_avg_var=fsgen_weights['bn1/avg_var'])\n",
    "            self.bn2 = L.BatchNormalization(256, use_beta=False,initial_gamma=fsgen_weights['bn2/gamma'], initial_avg_mean=fsgen_weights['bn2/avg_mean'], initial_avg_var=fsgen_weights['bn2/avg_var'])\n",
    "            self.bn3 = L.BatchNormalization(128, use_beta=False,initial_gamma=fsgen_weights['bn3/gamma'], initial_avg_mean=fsgen_weights['bn3/avg_mean'], initial_avg_var=fsgen_weights['bn3/avg_var'])\n",
    "            self.bn4 = L.BatchNormalization(128, use_beta=False,initial_gamma=fsgen_weights['bn4/gamma'], initial_avg_mean=fsgen_weights['bn4/avg_mean'], initial_avg_var=fsgen_weights['bn4/avg_var'])\n",
    "            \n",
    "            slow_mid_dim = bottom_width * bottom_width * conv_ch // 2\n",
    "            fast_mid_dim = slow_mid_dim\n",
    "            \n",
    "            self.l0s = L.Linear(100, slow_mid_dim, initialW=vgen_weights['l0s/W'], nobias=True)\n",
    "            self.l0f = L.Linear(100, fast_mid_dim, initialW=vgen_weights['l0f/W'], nobias=True)\n",
    "            self.dcg1 = L.Deconvolution2D(conv_ch, conv_ch // 2, 4, 2, 1, initialW=vgen_weights['dc1/W'], nobias=True)\n",
    "            self.dcg2 = L.Deconvolution2D(conv_ch // 2, conv_ch // 4, 4, 2, 1, initialW=vgen_weights['dc2/W'], nobias=True)\n",
    "            self.dcg3 = L.Deconvolution2D(conv_ch // 4, conv_ch // 8, 4, 2, 1, initialW=vgen_weights['dc3/W'], nobias=True)\n",
    "            self.dcg4 = L.Deconvolution2D(conv_ch // 8, conv_ch // 16, 4, 2, 1, initialW=vgen_weights['dc4/W'], nobias=True)\n",
    "            self.dcg5 = L.Deconvolution2D(conv_ch // 16, 1, 3, 1, 1, initialW=vgen_weights['dc5/W'], initial_bias=vgen_weights['dc5/b'])\n",
    "            self.bn0s = L.BatchNormalization(slow_mid_dim,initial_gamma=vgen_weights['bn0s/gamma'], initial_beta=vgen_weights['bn0s/beta'], initial_avg_mean=vgen_weights['bn0s/avg_mean'], initial_avg_var=vgen_weights['bn0s/avg_var'])\n",
    "            self.bn0f = L.BatchNormalization(fast_mid_dim,initial_gamma=vgen_weights['bn0s/gamma'], initial_beta=vgen_weights['bn0f/beta'], initial_avg_mean=vgen_weights['bn0f/avg_mean'], initial_avg_var=vgen_weights['bn0f/avg_var'])\n",
    "            self.bng1 = L.BatchNormalization(conv_ch // 2,initial_gamma=vgen_weights['bn1/gamma'], initial_beta=vgen_weights['bn1/beta'], initial_avg_mean=vgen_weights['bn1/avg_mean'], initial_avg_var=vgen_weights['bn1/avg_var'])\n",
    "            self.bng2 = L.BatchNormalization(conv_ch // 4,initial_gamma=vgen_weights['bn2/gamma'], initial_beta=vgen_weights['bn2/beta'], initial_avg_mean=vgen_weights['bn2/avg_mean'], initial_avg_var=vgen_weights['bn2/avg_var'])\n",
    "            self.bng3 = L.BatchNormalization(conv_ch // 8,initial_gamma=vgen_weights['bn3/gamma'], initial_beta=vgen_weights['bn3/beta'], initial_avg_mean=vgen_weights['bn3/avg_mean'], initial_avg_var=vgen_weights['bn3/avg_var'])\n",
    "            self.bng4 = L.BatchNormalization(conv_ch // 16,initial_gamma=vgen_weights['bn4/gamma'], initial_beta=vgen_weights['bn4/beta'], initial_avg_mean=vgen_weights['bn4/avg_mean'], initial_avg_var=vgen_weights['bn4/avg_var'])\n",
    "\n",
    "    def forward(self, z_slow):\n",
    "        #Adversary layers\n",
    "        h = F.reshape(F.flatten(z_slow),[1,-1])\n",
    "        h = F.relu(self.l1(h))\n",
    "        z_slow = F.tanh(self.l2(h))\n",
    "        \n",
    "        with chainer.using_config('train', False):\n",
    "            #Frame seed generation\n",
    "            h = F.reshape(z_slow, (h.shape[0], -1, 1))\n",
    "            h = F.relu(self.bn0(self.dc0(h)))\n",
    "            h = F.relu(self.bn1(self.dc1(h)))\n",
    "            h = F.relu(self.bn2(self.dc2(h)))\n",
    "            h = F.relu(self.bn3(self.dc3(h)))\n",
    "            h = F.relu(self.bn4(self.dc4(h)))\n",
    "            z_fast = F.tanh(self.dc5(h))\n",
    "            \n",
    "            #Prep for video generation\n",
    "            B, n_z_fast, n_frames = z_fast.shape\n",
    "            z_fast = F.reshape(z_fast, (B * n_frames, n_z_fast)) #Reshape from (batch, frames, z_dim) to (batch*frames,z_dim)\n",
    "\n",
    "            B, n_z_slow = z_slow.shape\n",
    "            z_slow = F.reshape(F.broadcast_to(F.reshape(z_slow, (B, 1, n_z_slow)), (B, n_frames, n_z_slow)),(B * n_frames, n_z_slow))\n",
    "        \n",
    "            #Video generation\n",
    "            n = z_slow.shape[0]\n",
    "            h_slow = F.reshape(F.relu(self.bn0s(self.l0s(z_slow))), [n, 512 // 2, 4, 4])\n",
    "            h_fast = F.reshape(F.relu(self.bn0f(self.l0f(z_fast))), [n, 512 // 2, 4, 4])\n",
    "            h = F.concat([h_slow, h_fast], axis=1)\n",
    "            h = F.relu(self.bng1(self.dcg1(h)))\n",
    "            h = F.relu(self.bng2(self.dcg2(h)))\n",
    "            h = F.relu(self.bng3(self.dcg3(h)))\n",
    "            h = F.relu(self.bng4(self.dcg4(h)))\n",
    "            x = F.tanh(self.dcg5(h))\n",
    "            \n",
    "        return x\n",
    "\n",
    "gpu_id = 0  # Set to -1 if you use CPU\n",
    "\n",
    "model = MLP()\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 \tL: 125.403564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\chainer\\functions\\connection\\convolution_2d.py:295: PerformanceWarning: The best algo of conv fwd might not be selected due to lack of workspace size (8388608)\n",
      "  cuda.cudnn.convolution_forward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 \tL: 2.2297584e-08\n",
      "iteration: 200 \tL: 2.2099112e-08\n",
      "iteration: 300 \tL: 2.1867137e-08\n",
      "iteration: 400 \tL: 2.1583194e-08\n",
      "iteration: 500 \tL: 2.119791e-08\n",
      "iteration: 600 \tL: 2.069314e-08\n",
      "iteration: 700 \tL: 1.9986924e-08\n",
      "iteration: 800 \tL: 1.8650727e-08\n",
      "iteration: 900 \tL: 1.5993603e-08\n",
      "min loss: 1.1570318e-08\n",
      "iteration: 0 \tL: 839.6297\n",
      "iteration: 100 \tL: 839.6297\n",
      "iteration: 200 \tL: 3.1877759\n",
      "iteration: 300 \tL: 1.1888638\n",
      "iteration: 400 \tL: 0.77088904\n",
      "iteration: 500 \tL: 0.599983\n",
      "iteration: 600 \tL: 0.5145745\n",
      "iteration: 700 \tL: 0.44712514\n",
      "iteration: 800 \tL: 0.3961349\n",
      "iteration: 900 \tL: 0.5724473\n",
      "min loss: 0.34007484\n",
      "iteration: 0 \tL: 710.2913\n",
      "iteration: 100 \tL: 1.7551603\n",
      "iteration: 200 \tL: 0.86011547\n",
      "iteration: 300 \tL: 0.59600806\n",
      "iteration: 400 \tL: 0.47297555\n",
      "iteration: 500 \tL: 0.40370864\n",
      "iteration: 600 \tL: 0.383119\n",
      "iteration: 700 \tL: 0.3343531\n",
      "iteration: 800 \tL: 0.31478897\n",
      "iteration: 900 \tL: 0.31081462\n",
      "min loss: 0.27709818\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 1000\n",
    "fsgen_weights = np.load('C:/Users/robbe/tgan-master/tgan-master/results/PrivacyTest-last20witheld-unconditional/gen_iter_28000.npz')\n",
    "vgen_weights = np.load('C:/Users/robbe/tgan-master/tgan-master/results/PrivacyTest-last20witheld-unconditional/vgen_iter_28000.npz')\n",
    "cr_imgs = np.load('C:/Users/robbe/tgan-master/tgan-master/cropped_imgs.npy')\n",
    "batchsize = 1\n",
    "min_loss_array = np.empty(201)\n",
    "\n",
    "model = MLP()\n",
    "model.to_gpu(gpu_id)\n",
    "\n",
    "def loss_fn(x, y):\n",
    "  y = F.transpose(y,[1,0,2,3])\n",
    "  d = x - (y-128)/128\n",
    "  return F.sum(d * d)\n",
    "\n",
    "for i in range(5,8):\n",
    "    train = np.expand_dims(cr_imgs[16:-16,i,:,:],axis=1)\n",
    "    train = np.transpose(train,[1,0,2,3]) #shape is (n,z,x,y)\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "\n",
    "    # selection of your optimizing method\n",
    "    optimizer = optimizers.Adam(alpha=0.0001)\n",
    "\n",
    "    # Give the optimizer a reference to the model\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # Get an updater that uses the Iterator and Optimizer\n",
    "    updater = training.updaters.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "\n",
    "    it = -1\n",
    "    min_loss = 1e5\n",
    "    while train_iter.epoch < max_epoch:\n",
    "        it=it+1\n",
    "        # ---------- The first iteration of training loop ----------\n",
    "        train_batch = train_iter.next()\n",
    "        cv = chainer.Variable(np.array(train_batch,dtype='float32'))\n",
    "        cv.to_gpu()\n",
    "    #     target = chainer.Variable(np.random.rand(1,32,64,64).astype('float32'))\n",
    "    #     target.to_gpu()\n",
    "\n",
    "        # calculate the prediction of the model\n",
    "        pred = model(cv)\n",
    "\n",
    "        # calculation of loss function, softmax_cross_entropy\n",
    "        loss = loss_fn(pred, cv)\n",
    "        if it%100==0:\n",
    "            print('iteration:',it,'\\tL:',loss.data)\n",
    "        if min_loss > loss.data:\n",
    "            min_loss=loss.data\n",
    "        # calculate the gradients in the model\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters of the model\n",
    "        optimizer.update()\n",
    "    min_loss_array[i] = min_loss\n",
    "    print('min loss:', min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.15703180e-08, 3.40074837e-01, 2.77098179e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chainer.training import extensions\n",
    "# trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='chainer_test')\n",
    "\n",
    "# trainer.extend(extensions.LogReport())\n",
    "# trainer.extend(extensions.snapshot(filename='snapshot_epoch-{.updater.epoch}'))\n",
    "# # trainer.extend(extensions.snapshot_object(model.predictor, filename='model_epoch-{.updater.epoch}'))\n",
    "# #trainer.extend(extensions.Evaluator(test_iter, model, device=gpu_id))\n",
    "# trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'elapsed_time']))\n",
    "# trainer.extend(extensions.PlotReport(['main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "# trainer.extend(extensions.PlotReport(['main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "# trainer.extend(extensions.DumpGraph('main/loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f286e7f3ec84ed69cf40636e7afbc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Slice plane selection:', options=('x-y', 'y-z', 'z-x'), style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f6a9e88c1640e5b8e19e4ba595e44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Slice plane selection:', options=('x-y', 'y-z', 'z-x'), style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ImageViewer3D.ImageSliceViewer3D at 0x206895b5b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(cv)\n",
    "cv.to_cpu()\n",
    "out.to_cpu()\n",
    "\n",
    "target = (np.array(cv.data,dtype='float32')[0,:,:,:]-128)/128\n",
    "I3D.ImageSliceViewer3D(np.array(out.data,dtype='float32')[:,0,:,:]*20+20)\n",
    "I3D.ImageSliceViewer3D(target*20+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
